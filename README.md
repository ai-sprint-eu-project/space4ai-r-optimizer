# SPACE4AI-R Optimizer

The tool is implemented in C++, with a Python-based entrypoint that is in 
charge of managing the input/output files according to the scenario under 
testing.

It relies on performance models developed and queried through the 
[aMLLibrary](https://github.com/aMLLibrary/aMLLibrary), specifying a 
regressor file that has to be suitably generated for each application 
component and computing continuum device.

## Deployment instructions

### Prerequisites

The `SPACE4AI-R` optimizer works under the following assumptions:
* An `aMLLibrary` container is deployed and accessible by HTTP requests 
coming to a specific address and port.
* The `AMLLIBRARY_URL` and `AMLLIBRARY_PORT` environment variables are set so 
that `POST` requests can be issued to 
`http://<AMLLIBRARY_URL>:<AMLLIBRARY_PORT>/amllibrary/predict`, sending the 
data required to query a specific performance model. 
* A shared volume is available and accessible by both the `SPACE4AI-R` and the 
`aMLLibrary` container to access the regressor files needed by the performance 
models.
* The mount point of the shared volume is set in an environment variable 
`MOUNT_POINT` when creating the container (default: `/mnt`).

### Docker image generation

When building the docker image for the `SPACE4AI-R` optimizer, this is 
installed in a `/home/SPACE4AI-R` directory, which includes 
the Python entrypoint together with the C++ optimizer code. The latter is 
compiled upon image generation; the main executable is 
`/home/SPACE4AI-R/s4ai-optimizer/BUILD/s4air_exe`.

The image should be generated by executing different commands, depending on 
whether you intend to use it for development or in a production scenario.

Specifically, the `image-dev` and `image-prod` scenarios differ on the 
version of the 
[SPACE4AI-Parser](https://gitlab.polimi.it/ai-sprint/space4ai-parser) 
that is considered. Indeed, when the docker image is generated, the parser 
code is cloned into the `/home/SPACE4AI-R` directory. This will be used by 
the entrypoint to generate the input/output files in the format required by 
the optimizer.

#### Docker image for development

When building under the `image-dev` setting, the most recent version of the 
parser is cloned from the corresponding git repository. The docker image 
should be generated as follows:

```
IMG_TARGET=image-dev
IMG_NAME=aisprint/space4ai-r
IMG_TAG=dev
docker build --target ${IMG_TARGET} -t ${IMG_NAME}:${IMG_TAG} .
```

#### Docker image for production

When using the `image-prod` target, instead, a specific version of the 
SPACE4AI Parser is considered. By default, executing the following command, 
the latest tagged version of the parser is considered:

```
IMG_TARGET=image-prod
IMG_NAME=aisprint/space4ai-r
IMG_TAG=23.06.29b
docker build --target ${IMG_TARGET} -t ${IMG_NAME}:${IMG_TAG} .
```

To consider a specific 
[parser version](https://gitlab.polimi.it/ai-sprint/space4ai-parser/-/tags), 
run:

```
IMG_TARGET=image-prod
IMG_NAME=aisprint/space4ai-r
IMG_TAG=23.06.29b
PARSER_TAG=23.06.30
docker build  --target ${IMG_TARGET} \
              --build-arg PARSER_TAG=${PARSER_TAG} \
              -t ${IMG_NAME}:${IMG_TAG} .
```

### Start the container

To start the docker container in interactive mode, run:

```
CONT_NAME=s4airopt
PATH_TO_VOLUME=${PWD}/example_applications
MOUNT_POINT=/mnt
docker run  -it \
            --name ${CONT_NAME} \
            -e MOUNT_POINT=${MOUNT_POINT} \
            -v ${PATH_TO_VOLUME}:${MOUNT_POINT} \
            ${IMG_NAME}:${IMG_TAG}
```

where `PATH_TO_VOLUME` is the (global) path to the volume where the 
application directory is stored. Examples of the application directory can 
be downloaded from the 
[AI-SPRINT Examples](https://gitlab.polimi.it/ai-sprint/ai-sprint-examples) 
following the instructions [here](example_applications/README.md).

#### Running in Kubernetes 

If you are working with Kubernetes, the optimizer image needs to be pushed to 
a docker registry, or, if executing on a 
[kind]([https://kind.sigs.k8s.io](https://kind.sigs.k8s.io/)) cluster, loaded 
into the cluster local registry. To do so, run:

```
kind load docker image ${IMG_NAME}:${IMG_TAG}
```

Once the image is loaded into the cluster registry, you can set the 
`ImagePullPolicy` in the pod specification to `Never`.

An example of pod generated to run the SPACE4AI-D Optimizer is provided in 
[s4air-optimizer-pod.yaml](k8s/s4air-optimizer-pod.yaml), and can be started 
by running:

```
kubectl apply -f k8s/s4air-optimizer-pod.yaml
```

#### Starting the Web API to get the maximum admissible workload

To expose the Web interface to get the maximum admissible workload for the 
current production deployment, run:

```
CONT_NAME=s4airapi
PATH_TO_VOLUME=${PWD}/example_applications
MOUNT_POINT=/mnt
docker run  --rm \
            --name ${CONT_NAME} \
            -e MOUNT_POINT=${MOUNT_POINT} \
            -v ${PATH_TO_VOLUME}:${MOUNT_POINT} \
            -p 8008:8008 \
            ${IMG_NAME}:${IMG_TAG} \
            python3 maximum_workload.py
```

or, if working with Kubernetes, apply the suitable deployment and service by 
executing:

```
kubectl apply -k k8s/
```

(this relies on the [Kustomize](https://kustomize.io) configuration manager 
and creates all the resources specified in the 
[kustomization.yaml](k8s/kustomization.yaml) file).

Instructions to call the API are provided in the following.

## Execution instructions

The entrypoint of the `SPACE4AI-R` optimizer is the `s4ai-r-opt.py` script, 
that is called from the command-line with the following prototype:

```
usage: s4ai-r-opt.py [-h] [--application_dir APPLICATION_DIR] [--load LOAD]
                     [--on_edge] [--RG_n_iterations RG_N_ITERATIONS]
                     [--LS_n_iterations LS_N_ITERATIONS]
                     [--max_num_sols MAX_NUM_SOLS] [--drop_seed]
                     [--verbosity_level {INFO,DEBUG,TRACE}] [--log_on_file]

SPACE4AI-R optimizer

optional arguments:
  -h, --help            show this help message and exit
  --application_dir APPLICATION_DIR
                        Path to the application directory
  --load LOAD           Input workload
  --on_edge             True if the optimizer should consider only edge
                        resources
  --RG_n_iterations RG_N_ITERATIONS
                        Number of iterations for the RandomGreedy algorithm
  --LS_n_iterations LS_N_ITERATIONS
                        Number of iterations for the LocalSearch algorithm
  --max_num_sols MAX_NUM_SOLS
                        Maximum number of elite solutions to be saved
  --drop_seed           True to avoid using a fixed seed for random number
                        generation
  --verbosity_level {INFO,DEBUG,TRACE}
                        Verbosity level for logging
  --log_on_file         True to print logging info to a s4ai-r-LOG.log file
```

The required parameters to this script are:
* The path to the application directory, which is assumed to have the 
following structure:
* The input workload for the application at the time the optimizer is called 
(in requests per second). Based on this value, the system description, the 
current production deployment and the expected response times of components, 
the configuration is updated to minimise costs while meeting QoS local and 
global constraints.

Additionally, you may specify:
* `on_edge`, if the optimizer is called at the edge level after a network 
disconnection. This is needed to effectively design the input files and 
consequently generate the best configuration update.

Finally, specifying the next set of parameters overrides the algorithm default 
configuration, changing:
- The number of iterations for the `RandomGreedy` algorithm (default: 1000).
- The number of iterations for the `LocalSearch` algorithm, executed after 
`RandomGreedy` on the set of elite solutions it generates (default: 10).
- The maximum number of elite solutions saved while running the `RandomGreedy` 
algorithm and therefore provided as input to `LocalSearch` (default: 10).
- `drop_seed`, if you want to avoid using a fixed seed for random number 
generations, thus achieving complete stochasticiy (**note:** this of course 
entails that successive runs exploiting the same input parameters are not 
guaranteed to produce the same results, i.e., reproducibility is not enforced).
- the verbosity level of logging messages, which may be:
	- `INFO`: used to log events within the parameters of expected program 
  behavior.
  - `DEBUG`: used to log messages that are useful for debugging.
  - `TRACE`: used to more detailed messages about the program execution and 
  status.
- `log_on_file` to redirect logged messages to a `s4ai-r-LOG.log` file.

Once started, the script parses the input YAML files available in the 
application directory to generate the JSON inputs consumed by the C++ 
optimizer, differentiating the case where the system includes both edge and 
cloud or only edge resources based on the value of the input parameter 
provided by the caller.

### Call the maximum workload API

A script to call the maximum workload API are provided in 
[test_webapi.py](example_applications/test_webapi.py). The application 
directory and workload interval bounds are passed to the script as 
parameters. Sample values, relying on the 
[blurry faces example](https://gitlab.polimi.it/ai-sprint/ai-sprint-examples/-/tree/main/blurry_faces_single_component_local_constraint/step_6), 
are provided [here](k8s/test-max-load-api-pod.yaml) and can be executed 
by applying:

```
kubectl apply -f k8s/test-max-load-api-pod.yaml
```

if the required resources are already in place.

**Warning:** the maximum workload API relies on an input json file produced 
when running the SPACE4AI-R optimizer for the first time; therefore, errors 
will be incurred if the API is called before running any runtime 
reconfiguration.
